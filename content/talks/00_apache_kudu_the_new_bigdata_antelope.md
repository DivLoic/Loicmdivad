---
title: "Apache Kudu: the new big data antelope"
date: 2017-04-10T09:00:00+01:00
draft: false
author: "Loïc DIVAD"
tags: [ "data", "distributed" ]
---

The abstract:
> Apache Hadoop and it's distributed file system are probably the most representative to tools in the Big Data Area. They have democratised distributed workloads on large datasets for hundreds of companies already, just in Paris. But these workloads are append-only batches. However, life in companies can't be only described by fast scan systems. We need writes but also updates. We need fast random access and mutable data on huge dataset too. And when it comes to supporting continuous ingestions of event, mix HDSF, and columnar database (e.g., HBase or Cassandra) within the same architecture may be complicated. To solve all these issues, I will present Apache Kudu, a fast analytics storage layer.

The slides:
{{< speakerdeck id=e50c5340a2254583a676d84f15ffe7b6 slide=0 >}}  
  
_Apache Kudu: the new big data antelope_ was my first XKE (Xebia Knowledge Exchange) and firts technical talk ever!
Again, Thank you ❤️ [Publicis Sapient Engineering](https://medium.com/xebia-france) (formerly Xebia France).  

![](/images/talks/00_apache_kudu.jpg)